{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 01 — Data Profiling & Statistical Analysis (BTCUSDT)\n",
    "\n",
    "**Goal:** Understand the statistical properties of our market data before building signals.\n",
    "\n",
    "**Questions:**\n",
    "1. Return distributions at 1m/5m — tail behavior, skewness, kurtosis\n",
    "2. Autocorrelation of returns — is there serial dependence?\n",
    "3. Volatility clustering — how persistent are vol regimes?\n",
    "4. Intraday seasonality — volume/volatility by hour of day\n",
    "5. Cross-exchange correlation — price relationships & lead-lag across Binance, Bybit, OKX\n",
    "6. Volume imbalance — predictive of forward returns?\n",
    "7. Binance metrics — OI, funding, L/S ratios\n",
    "\n",
    "**Data:** Daily-partitioned parquet files from `build_parquet.py`  \n",
    "**Period:** 2025-11-01 → 2026-01-31 (92 days)  \n",
    "**Sources:** Bybit, Binance, OKX (futures + spot each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "PARQUET_DIR = Path('../parquet')\n",
    "SYMBOL = 'BTCUSDT'\n",
    "\n",
    "SOURCES = ['bybit_futures', 'bybit_spot', 'binance_futures', 'binance_spot',\n",
    "           'okx_futures', 'okx_spot']\n",
    "FUTURES_SOURCES = ['bybit_futures', 'binance_futures', 'okx_futures']\n",
    "SPOT_SOURCES = ['bybit_spot', 'binance_spot', 'okx_spot']\n",
    "\n",
    "SOURCE_LABELS = {\n",
    "    'bybit_futures': 'Bybit Futures',\n",
    "    'bybit_spot': 'Bybit Spot',\n",
    "    'binance_futures': 'Binance Futures',\n",
    "    'binance_spot': 'Binance Spot',\n",
    "    'okx_futures': 'OKX Futures',\n",
    "    'okx_spot': 'OKX Spot',\n",
    "}\n",
    "SOURCE_COLORS = {\n",
    "    'bybit_futures': '#FF6B00',\n",
    "    'bybit_spot': '#FFB366',\n",
    "    'binance_futures': '#F0B90B',\n",
    "    'binance_spot': '#F0D96B',\n",
    "    'okx_futures': '#00C4B4',\n",
    "    'okx_spot': '#66E0D6',\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Loaders for daily-partitioned parquet\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_ohlcv(symbol, interval, source):\n",
    "    \"\"\"Load all daily OHLCV parquet files for a source, concatenated.\"\"\"\n",
    "    ohlcv_dir = PARQUET_DIR / symbol / 'ohlcv' / interval / source\n",
    "    if not ohlcv_dir.exists():\n",
    "        return pd.DataFrame()\n",
    "    files = sorted(ohlcv_dir.glob('*.parquet'))\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "    df = df.sort_values('timestamp_us').reset_index(drop=True)\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp_us'], unit='us', utc=True)\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    return df\n",
    "\n",
    "def load_ohlcv_daterange(symbol, interval, source, start_date, end_date):\n",
    "    \"\"\"Load OHLCV for a specific date range (inclusive).\"\"\"\n",
    "    ohlcv_dir = PARQUET_DIR / symbol / 'ohlcv' / interval / source\n",
    "    if not ohlcv_dir.exists():\n",
    "        return pd.DataFrame()\n",
    "    dates = pd.date_range(start_date, end_date)\n",
    "    files = [ohlcv_dir / f'{d.strftime(\"%Y-%m-%d\")}.parquet' for d in dates]\n",
    "    files = [f for f in files if f.exists()]\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "    df = df.sort_values('timestamp_us').reset_index(drop=True)\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp_us'], unit='us', utc=True)\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    return df\n",
    "\n",
    "def load_metrics(symbol):\n",
    "    \"\"\"Load all daily Binance futures metrics.\"\"\"\n",
    "    metrics_dir = PARQUET_DIR / symbol / 'binance' / 'metrics'\n",
    "    if not metrics_dir.exists():\n",
    "        return pd.DataFrame()\n",
    "    files = sorted(metrics_dir.glob('*.parquet'))\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "    df = df.sort_values('timestamp_us').reset_index(drop=True)\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp_us'], unit='us', utc=True)\n",
    "    return df\n",
    "\n",
    "def count_trades(symbol, source):\n",
    "    \"\"\"Count total trades and get date range without loading all data.\"\"\"\n",
    "    trades_dir = PARQUET_DIR / symbol / 'trades' / source\n",
    "    if not trades_dir.exists():\n",
    "        return 0, None, None, None, None\n",
    "    files = sorted(trades_dir.glob('*.parquet'))\n",
    "    if not files:\n",
    "        return 0, None, None, None, None\n",
    "    total = 0\n",
    "    price_min, price_max = float('inf'), float('-inf')\n",
    "    for f in files:\n",
    "        df = pd.read_parquet(f, columns=['price'])\n",
    "        total += len(df)\n",
    "        price_min = min(price_min, df['price'].min())\n",
    "        price_max = max(price_max, df['price'].max())\n",
    "    first_date = files[0].stem\n",
    "    last_date = files[-1].stem\n",
    "    return total, first_date, last_date, price_min, price_max\n",
    "\n",
    "print(f'Ready. Symbol: {SYMBOL}')\n",
    "print(f'Parquet dir: {PARQUET_DIR / SYMBOL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Data Overview\n",
    "\n",
    "Quick summary of what we have: row counts, time ranges, price ranges per source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"=\" * 80}')\n",
    "print(f'  {SYMBOL} — Data Overview')\n",
    "print(f'{\"=\" * 80}')\n",
    "\n",
    "# Trades summary\n",
    "print(f'\\n  Raw Trades:')\n",
    "print(f'  {\"Source\":25s} {\"Trades\":>14s} {\"Date Range\":>25s} {\"Price Range\":>25s}')\n",
    "print(f'  {\"─\" * 92}')\n",
    "for src in SOURCES:\n",
    "    total, d0, d1, pmin, pmax = count_trades(SYMBOL, src)\n",
    "    if total == 0:\n",
    "        print(f'  {SOURCE_LABELS[src]:25s}  (no data)')\n",
    "        continue\n",
    "    print(f'  {SOURCE_LABELS[src]:25s} {total:>14,} {d0} → {d1:>10s} '\n",
    "          f'${pmin:>10,.2f} – ${pmax:,.2f}')\n",
    "\n",
    "# OHLCV summary\n",
    "for interval in ['1m', '1h']:\n",
    "    print(f'\\n  OHLCV {interval}:')\n",
    "    print(f'  {\"Source\":25s} {\"Bars\":>10s} {\"Total Volume\":>16s} {\"Total Trades\":>14s}')\n",
    "    print(f'  {\"─\" * 70}')\n",
    "    for src in SOURCES:\n",
    "        df = load_ohlcv(SYMBOL, interval, src)\n",
    "        if df.empty:\n",
    "            print(f'  {SOURCE_LABELS[src]:25s}  (no data)')\n",
    "            continue\n",
    "        print(f'  {SOURCE_LABELS[src]:25s} {len(df):>10,} {df[\"volume\"].sum():>16,.0f} '\n",
    "              f'{df[\"trade_count\"].sum():>14,}')\n",
    "\n",
    "# Metrics summary\n",
    "metrics = load_metrics(SYMBOL)\n",
    "if not metrics.empty:\n",
    "    print(f'\\n  Binance Metrics: {len(metrics):,} rows  '\n",
    "          f'{metrics[\"datetime\"].min().strftime(\"%Y-%m-%d\")} → '\n",
    "          f'{metrics[\"datetime\"].max().strftime(\"%Y-%m-%d\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Return Distributions\n",
    "\n",
    "Analyze return distributions at 1m and 5m to understand:\n",
    "- **Kurtosis** (fat tails = more extreme moves than normal)\n",
    "- **Skewness** (asymmetry = directional bias)\n",
    "- How far from Gaussian the returns are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "return-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{SYMBOL} — Return Statistics')\n",
    "print(f'{\"─\" * 100}')\n",
    "print(f'{\"Source\":25s} {\"Interval\":>8s} {\"Mean\":>12s} {\"Std\":>12s} '\n",
    "      f'{\"Skew\":>8s} {\"Kurt\":>8s} {\"Min\":>10s} {\"Max\":>10s} {\"JB p-val\":>10s}')\n",
    "print(f'{\"─\" * 100}')\n",
    "\n",
    "for interval in ['1m', '5m']:\n",
    "    for src in SOURCES:\n",
    "        df = load_ohlcv(SYMBOL, interval, src)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        r = df['returns'].dropna()\n",
    "        jb_stat, jb_p = stats.jarque_bera(r)\n",
    "        print(f'{SOURCE_LABELS[src]:25s} {interval:>8s} '\n",
    "              f'{r.mean():>12.6f} {r.std():>12.6f} '\n",
    "              f'{r.skew():>8.3f} {r.kurtosis():>8.2f} '\n",
    "              f'{r.min():>10.5f} {r.max():>10.5f} '\n",
    "              f'{jb_p:>10.2e}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "return-histograms",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(f'{SYMBOL} — Return Distributions (1m top, 5m bottom)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, interval in enumerate(['1m', '5m']):\n",
    "    for j, src in enumerate(FUTURES_SOURCES):\n",
    "        ax = axes[i, j]\n",
    "        df = load_ohlcv(SYMBOL, interval, src)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        r = df['returns'].dropna()\n",
    "        \n",
    "        ax.hist(r, bins=200, density=True, alpha=0.7,\n",
    "                color=SOURCE_COLORS[src], label=SOURCE_LABELS[src])\n",
    "        \n",
    "        # Normal overlay\n",
    "        x = np.linspace(r.quantile(0.001), r.quantile(0.999), 300)\n",
    "        ax.plot(x, stats.norm.pdf(x, r.mean(), r.std()),\n",
    "                'r--', alpha=0.8, label='Normal')\n",
    "        \n",
    "        ax.set_title(f'{SOURCE_LABELS[src]} — {interval} returns')\n",
    "        ax.set_xlabel('Return')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.set_xlim(-0.01, 0.01)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Autocorrelation of Returns\n",
    "\n",
    "If returns have significant autocorrelation at short lags, there's direct evidence of predictability.\n",
    "- Positive ACF → momentum\n",
    "- Negative ACF → mean reversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "autocorrelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(f'{SYMBOL} — Autocorrelation of Returns', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, interval in enumerate(['1m', '5m']):\n",
    "    for j, src in enumerate(FUTURES_SOURCES):\n",
    "        ax = axes[i, j]\n",
    "        df = load_ohlcv(SYMBOL, interval, src)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        r = df['returns'].dropna().values\n",
    "        \n",
    "        nlags = min(60, len(r) // 4)\n",
    "        acf_vals, confint = acf(r, nlags=nlags, alpha=0.05)\n",
    "        \n",
    "        ax.bar(range(nlags + 1), acf_vals, width=0.6,\n",
    "               color=SOURCE_COLORS[src], alpha=0.7)\n",
    "        ax.fill_between(range(nlags + 1),\n",
    "                       confint[:, 0] - acf_vals,\n",
    "                       confint[:, 1] - acf_vals,\n",
    "                       alpha=0.15, color='blue')\n",
    "        ax.axhline(0, color='black', linewidth=0.5)\n",
    "        ax.set_title(f'{SOURCE_LABELS[src]} — {interval} returns ACF')\n",
    "        ax.set_xlabel('Lag')\n",
    "        ax.set_ylabel('ACF')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ACF of absolute returns (volatility clustering)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "fig.suptitle(f'{SYMBOL} — ACF of |Returns| (Volatility Clustering, 1m)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for j, src in enumerate(FUTURES_SOURCES):\n",
    "    ax = axes[j]\n",
    "    df = load_ohlcv(SYMBOL, '1m', src)\n",
    "    if df.empty:\n",
    "        continue\n",
    "    r = df['returns'].dropna().abs().values\n",
    "    nlags = min(120, len(r) // 4)\n",
    "    acf_vals, confint = acf(r, nlags=nlags, alpha=0.05)\n",
    "    \n",
    "    ax.bar(range(nlags + 1), acf_vals, width=0.6,\n",
    "           color=SOURCE_COLORS[src], alpha=0.7)\n",
    "    ax.fill_between(range(nlags + 1),\n",
    "                   confint[:, 0] - acf_vals,\n",
    "                   confint[:, 1] - acf_vals,\n",
    "                   alpha=0.15, color='blue')\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.set_title(f'{SOURCE_LABELS[src]} — 1m |returns| ACF')\n",
    "    ax.set_xlabel('Lag (minutes)')\n",
    "    ax.set_ylabel('ACF')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Intraday Seasonality\n",
    "\n",
    "Volume and volatility patterns by hour of day (UTC). Identifies:\n",
    "- When liquidity is highest (lower slippage)\n",
    "- When volatility is highest (more opportunity)\n",
    "- When to avoid trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonality",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f'{SYMBOL} — Intraday Seasonality (1m bars, by hour UTC)', fontsize=14, fontweight='bold')\n",
    "\n",
    "bar_width = 0.13\n",
    "offsets = {src: (i - 2.5) * bar_width for i, src in enumerate(SOURCES)}\n",
    "\n",
    "for src in SOURCES:\n",
    "    df = load_ohlcv(SYMBOL, '1m', src)\n",
    "    if df.empty:\n",
    "        continue\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    color = SOURCE_COLORS[src]\n",
    "    label = SOURCE_LABELS[src]\n",
    "    off = offsets[src]\n",
    "    \n",
    "    # Volume by hour\n",
    "    hourly_vol = df.groupby('hour')['volume'].mean()\n",
    "    axes[0].bar(hourly_vol.index + off, hourly_vol.values, width=bar_width,\n",
    "               color=color, alpha=0.8, label=label)\n",
    "    \n",
    "    # Volatility by hour\n",
    "    hourly_std = df.groupby('hour')['returns'].std()\n",
    "    axes[1].bar(hourly_std.index + off, hourly_std.values * 100, width=bar_width,\n",
    "               color=color, alpha=0.8, label=label)\n",
    "    \n",
    "    # Trade count by hour\n",
    "    hourly_trades = df.groupby('hour')['trade_count'].mean()\n",
    "    axes[2].bar(hourly_trades.index + off, hourly_trades.values, width=bar_width,\n",
    "               color=color, alpha=0.8, label=label)\n",
    "\n",
    "axes[0].set_title('Average Volume per 1m Bar')\n",
    "axes[1].set_title('Return Volatility (std %)')\n",
    "axes[2].set_title('Average Trade Count per 1m Bar')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Hour (UTC)')\n",
    "    ax.set_xticks(range(24))\n",
    "    ax.legend(fontsize=7, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Cross-Exchange Analysis\n",
    "\n",
    "Key questions across all 3 exchanges (Binance, Bybit, OKX):\n",
    "- How correlated are prices across exchanges?\n",
    "- Does one exchange lead the others? (lead-lag)\n",
    "- What's the typical price spread between them?\n",
    "- Futures vs spot basis per exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-exchange-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 1m OHLCV for all futures sources\n",
    "futures_1m = {}\n",
    "for src in FUTURES_SOURCES:\n",
    "    futures_1m[src] = load_ohlcv(SYMBOL, '1m', src)\n",
    "    print(f'  {SOURCE_LABELS[src]:20s}: {len(futures_1m[src]):,} bars')\n",
    "\n",
    "# Merge all futures on timestamp\n",
    "merged = futures_1m['binance_futures'][['timestamp_us', 'close', 'returns', 'volume']].rename(\n",
    "    columns={'close': 'close_bn', 'returns': 'ret_bn', 'volume': 'vol_bn'})\n",
    "\n",
    "for src, abbr in [('bybit_futures', 'bb'), ('okx_futures', 'okx')]:\n",
    "    merged = merged.merge(\n",
    "        futures_1m[src][['timestamp_us', 'close', 'returns', 'volume']].rename(\n",
    "            columns={'close': f'close_{abbr}', 'returns': f'ret_{abbr}', 'volume': f'vol_{abbr}'}),\n",
    "        on='timestamp_us', how='inner')\n",
    "\n",
    "merged['datetime'] = pd.to_datetime(merged['timestamp_us'], unit='us', utc=True)\n",
    "merged['spread_bn_bb'] = (merged['close_bn'] - merged['close_bb']) / merged['close_bn'] * 10000\n",
    "merged['spread_bn_okx'] = (merged['close_bn'] - merged['close_okx']) / merged['close_bn'] * 10000\n",
    "merged['spread_bb_okx'] = (merged['close_bb'] - merged['close_okx']) / merged['close_bb'] * 10000\n",
    "\n",
    "print(f'\\nMatched 1m bars across all 3 futures: {len(merged):,}')\n",
    "print(f'\\nPrice Correlations (close):')\n",
    "for a, b in [('bn', 'bb'), ('bn', 'okx'), ('bb', 'okx')]:\n",
    "    corr = merged[f'close_{a}'].corr(merged[f'close_{b}'])\n",
    "    print(f'  {a.upper():>4s} ↔ {b.upper():<4s}: {corr:.10f}')\n",
    "\n",
    "print(f'\\nReturn Correlations (1m):')\n",
    "for a, b in [('bn', 'bb'), ('bn', 'okx'), ('bb', 'okx')]:\n",
    "    corr = merged[f'ret_{a}'].corr(merged[f'ret_{b}'])\n",
    "    print(f'  {a.upper():>4s} ↔ {b.upper():<4s}: {corr:.6f}')\n",
    "\n",
    "print(f'\\nSpread Statistics (bps):')\n",
    "for col, label in [('spread_bn_bb', 'Binance-Bybit'), ('spread_bn_okx', 'Binance-OKX'), ('spread_bb_okx', 'Bybit-OKX')]:\n",
    "    s = merged[col]\n",
    "    print(f'  {label:16s}: mean={s.mean():+.3f}  std={s.std():.3f}  '\n",
    "          f'[{s.quantile(0.01):.2f}, {s.quantile(0.99):.2f}]')\n",
    "\n",
    "# --- Plots ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 9))\n",
    "fig.suptitle(f'{SYMBOL} — Cross-Exchange Futures (1m bars)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Row 1: Spread distributions\n",
    "for i, (col, label, color) in enumerate([\n",
    "    ('spread_bn_bb', 'Binance − Bybit', '#F0B90B'),\n",
    "    ('spread_bn_okx', 'Binance − OKX', '#00C4B4'),\n",
    "    ('spread_bb_okx', 'Bybit − OKX', '#FF6B00'),\n",
    "]):\n",
    "    ax = axes[0, i]\n",
    "    ax.hist(merged[col], bins=150, alpha=0.7, color=color, density=True)\n",
    "    ax.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f'{label} Spread (bps)')\n",
    "    ax.set_xlabel('bps')\n",
    "\n",
    "# Row 2: Return scatter plots\n",
    "for i, (a, b, color) in enumerate([\n",
    "    ('bn', 'bb', '#F0B90B'), ('bn', 'okx', '#00C4B4'), ('bb', 'okx', '#FF6B00')\n",
    "]):\n",
    "    ax = axes[1, i]\n",
    "    ax.scatter(merged[f'ret_{a}'] * 100, merged[f'ret_{b}'] * 100,\n",
    "              alpha=0.15, s=1, color=color)\n",
    "    lim = 0.5\n",
    "    ax.plot([-lim, lim], [-lim, lim], 'r--', alpha=0.5)\n",
    "    ax.set_xlim(-lim, lim)\n",
    "    ax.set_ylim(-lim, lim)\n",
    "    ax.set_title(f'{a.upper()} vs {b.upper()} 1m Returns')\n",
    "    ax.set_xlabel(f'{a.upper()} return (%)')\n",
    "    ax.set_ylabel(f'{b.upper()} return (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lead-lag",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead-lag analysis across all 3 futures exchanges\n",
    "print(f'{SYMBOL} — Lead-Lag Cross-Correlation (1m returns)')\n",
    "print(f'{\"─\" * 70}')\n",
    "\n",
    "pairs = [('bn', 'bb', 'Binance', 'Bybit'), ('bn', 'okx', 'Binance', 'OKX'), ('bb', 'okx', 'Bybit', 'OKX')]\n",
    "lags = range(-10, 11)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f'{SYMBOL} — Lead-Lag: Futures 1m Returns', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, (a, b, a_name, b_name) in enumerate(pairs):\n",
    "    ax = axes[idx]\n",
    "    a_leads = []\n",
    "    b_leads = []\n",
    "    \n",
    "    for lag in lags:\n",
    "        if lag == 0:\n",
    "            c = merged[f'ret_{a}'].corr(merged[f'ret_{b}'])\n",
    "            a_leads.append(c)\n",
    "            b_leads.append(c)\n",
    "        else:\n",
    "            c1 = merged[f'ret_{a}'].corr(merged[f'ret_{b}'].shift(-lag))\n",
    "            c2 = merged[f'ret_{b}'].corr(merged[f'ret_{a}'].shift(-lag))\n",
    "            a_leads.append(c1)\n",
    "            b_leads.append(c2)\n",
    "    \n",
    "    ax.bar([l - 0.15 for l in lags], a_leads, width=0.3, alpha=0.7,\n",
    "           color=SOURCE_COLORS[f'{a_name.lower()}_futures'],\n",
    "           label=f'{a_name} leads')\n",
    "    ax.bar([l + 0.15 for l in lags], b_leads, width=0.3, alpha=0.7,\n",
    "           color=SOURCE_COLORS[f'{b_name.lower()}_futures'],\n",
    "           label=f'{b_name} leads')\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.axvline(0, color='red', linestyle='--', alpha=0.3)\n",
    "    ax.set_xlabel('Lag (minutes)')\n",
    "    ax.set_ylabel('Cross-correlation')\n",
    "    ax.set_title(f'{a_name} vs {b_name}')\n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "    # Print key lags\n",
    "    print(f'\\n  {a_name} vs {b_name}:')\n",
    "    print(f'  {\"Lag\":>6s}  {f\"{a_name} leads\":>16s}  {f\"{b_name} leads\":>16s}')\n",
    "    for lag_i, lag in enumerate(lags):\n",
    "        if abs(lag) <= 3:\n",
    "            marker = ' ← contemp.' if lag == 0 else ''\n",
    "            print(f'  {lag:>6d}  {a_leads[lag_i]:>16.6f}  {b_leads[lag_i]:>16.6f}{marker}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Volume Imbalance Profiling\n",
    "\n",
    "Examine buy/sell volume ratio and its relationship to forward returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "volume-imbalance",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 9))\n",
    "fig.suptitle(f'{SYMBOL} — Volume Imbalance Analysis (1m, Futures)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for j, src in enumerate(FUTURES_SOURCES):\n",
    "    df = load_ohlcv(SYMBOL, '1m', src)\n",
    "    if df.empty:\n",
    "        continue\n",
    "    \n",
    "    df['volume_imbalance'] = (df['buy_volume'] - df['sell_volume']) / df['volume']\n",
    "    df['fwd_return_1m'] = df['returns'].shift(-1)\n",
    "    df['fwd_return_5m'] = df['close'].pct_change(5).shift(-5)\n",
    "    \n",
    "    clean = df.dropna(subset=['volume_imbalance', 'fwd_return_1m']).copy()\n",
    "    clean['imb_bucket'] = pd.qcut(clean['volume_imbalance'], q=10, labels=False, duplicates='drop')\n",
    "    \n",
    "    color = SOURCE_COLORS[src]\n",
    "    label = SOURCE_LABELS[src]\n",
    "    \n",
    "    # Imbalance distribution\n",
    "    axes[0, j].hist(clean['volume_imbalance'], bins=100, alpha=0.7, color=color)\n",
    "    axes[0, j].set_title(f'{label} — Imbalance Dist.')\n",
    "    axes[0, j].set_xlabel('(Buy - Sell) / Total')\n",
    "    \n",
    "    # Forward return by imbalance bucket\n",
    "    bucket_returns = clean.groupby('imb_bucket')['fwd_return_1m'].mean() * 10000\n",
    "    axes[1, j].bar(bucket_returns.index, bucket_returns.values, color=color, alpha=0.7)\n",
    "    axes[1, j].set_title(f'{label} — Fwd Return by Decile')\n",
    "    axes[1, j].set_xlabel('Imbalance Decile (0=sell, 9=buy)')\n",
    "    axes[1, j].set_ylabel('Fwd Return (bps)')\n",
    "    axes[1, j].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Correlations\n",
    "    corr_1m = clean['volume_imbalance'].corr(clean['fwd_return_1m'])\n",
    "    corr_5m = df.dropna(subset=['volume_imbalance', 'fwd_return_5m'])['volume_imbalance'].corr(\n",
    "        df.dropna(subset=['volume_imbalance', 'fwd_return_5m'])['fwd_return_5m'])\n",
    "    print(f'{label}: imbalance→fwd_1m corr={corr_1m:.6f}, imbalance→fwd_5m corr={corr_5m:.6f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. Metrics Analysis: Funding & Open Interest\n",
    "\n",
    "Examine Binance futures metrics for potential signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = load_metrics(SYMBOL)\n",
    "ohlcv = load_ohlcv(SYMBOL, '5m', 'binance_futures')\n",
    "\n",
    "if metrics.empty or ohlcv.empty:\n",
    "    print('Missing metrics or OHLCV data')\n",
    "else:\n",
    "    # Merge metrics with 5m OHLCV\n",
    "    merged_m = ohlcv[['timestamp_us', 'close', 'returns', 'volume']].merge(\n",
    "        metrics[['timestamp_us', 'open_interest', 'open_interest_value',\n",
    "                 'global_ls_ratio', 'top_trader_ls_ratio_positions']],\n",
    "        on='timestamp_us', how='inner'\n",
    "    )\n",
    "    merged_m['datetime'] = pd.to_datetime(merged_m['timestamp_us'], unit='us', utc=True)\n",
    "    merged_m['oi_change'] = merged_m['open_interest'].pct_change()\n",
    "    merged_m['fwd_return'] = merged_m['returns'].shift(-1)\n",
    "    \n",
    "    print(f'{SYMBOL} — Binance Futures Metrics ({len(merged_m):,} matched 5m bars)')\n",
    "    print(f'  OI range: {merged_m[\"open_interest\"].min():,.0f} – {merged_m[\"open_interest\"].max():,.0f}')\n",
    "    print(f'  Global L/S ratio: {merged_m[\"global_ls_ratio\"].min():.3f} – {merged_m[\"global_ls_ratio\"].max():.3f}')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'{SYMBOL} — Binance Futures Metrics', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # OI over time with price overlay\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(merged_m['datetime'], merged_m['open_interest'], color='steelblue', alpha=0.8)\n",
    "    ax1.set_title('Open Interest')\n",
    "    ax1.set_ylabel('Contracts', color='steelblue')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(merged_m['datetime'], merged_m['close'], color='orange', alpha=0.5, linewidth=0.5)\n",
    "    ax2.set_ylabel('Price', color='orange')\n",
    "    \n",
    "    # Global L/S ratio over time\n",
    "    axes[0, 1].plot(merged_m['datetime'], merged_m['global_ls_ratio'], color='green', alpha=0.8, linewidth=0.5)\n",
    "    axes[0, 1].axhline(1.0, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[0, 1].set_title('Global Long/Short Ratio')\n",
    "    axes[0, 1].set_ylabel('L/S Ratio')\n",
    "    \n",
    "    # OI change vs forward return\n",
    "    clean = merged_m.dropna(subset=['oi_change', 'fwd_return'])\n",
    "    axes[1, 0].scatter(clean['oi_change'] * 100, clean['fwd_return'] * 10000,\n",
    "                      alpha=0.3, s=3, color='steelblue')\n",
    "    axes[1, 0].set_title('OI Change vs 5m Forward Return')\n",
    "    axes[1, 0].set_xlabel('OI Change (%)')\n",
    "    axes[1, 0].set_ylabel('Forward Return (bps)')\n",
    "    axes[1, 0].set_xlim(-2, 2)\n",
    "    axes[1, 0].set_ylim(-100, 100)\n",
    "    axes[1, 0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # L/S ratio vs forward return\n",
    "    clean2 = merged_m.dropna(subset=['global_ls_ratio', 'fwd_return'])\n",
    "    axes[1, 1].scatter(clean2['global_ls_ratio'], clean2['fwd_return'] * 10000,\n",
    "                      alpha=0.3, s=3, color='green')\n",
    "    axes[1, 1].set_title('Global L/S Ratio vs 5m Forward Return')\n",
    "    axes[1, 1].set_xlabel('L/S Ratio')\n",
    "    axes[1, 1].set_ylabel('Forward Return (bps)')\n",
    "    axes[1, 1].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlations\n",
    "    print(f'\\nCorrelations with 5m forward return:')\n",
    "    for col in ['oi_change', 'global_ls_ratio', 'top_trader_ls_ratio_positions']:\n",
    "        if col in merged_m.columns:\n",
    "            c = merged_m[col].corr(merged_m['fwd_return'])\n",
    "            print(f'  {col:40s} {c:>10.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-summary",
   "metadata": {},
   "source": [
    "## Summary & Key Findings\n",
    "\n",
    "After running this notebook on BTCUSDT (92 days, 6 sources), document:\n",
    "\n",
    "1. **Return characteristics** — fat tails? skew? which timeframe has most structure?\n",
    "2. **Autocorrelation** — any significant lags? momentum or mean-reversion?\n",
    "3. **Volatility clustering** — how persistent? (informs position sizing)\n",
    "4. **Seasonality** — best hours to trade?\n",
    "5. **Cross-exchange** — which exchange leads? by how many minutes? spread distribution?\n",
    "6. **Volume imbalance** — predictive of forward returns? which exchange has strongest signal?\n",
    "7. **Metrics** — OI/L-S ratio signals worth pursuing?\n",
    "\n",
    "→ Proceed to `02_lead_lag.ipynb` for deeper cross-exchange analysis  \n",
    "→ Run same notebook for ETHUSDT and SOLUSDT once their parquet is built"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

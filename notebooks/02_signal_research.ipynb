{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f739bbe",
   "metadata": {},
   "source": [
    "# 02 — Signal Research: Microstructure Features from Tick Data\n",
    "\n",
    "**Goal:** Build features from raw tick trades that capture *what happened inside each candle* and evaluate their predictiveness for forward returns at 5m–1h horizons.\n",
    "\n",
    "**Feature Categories:**\n",
    "1. **Aggression** — taker buy/sell pressure, large trade detection\n",
    "2. **Flow patterns** — trade arrival rate, clustering, acceleration\n",
    "3. **Price impact** — how much price moves per unit of volume\n",
    "4. **Volume profile** — distribution of volume within the candle\n",
    "5. **Cross-exchange divergence** — when one venue sees unusual activity\n",
    "\n",
    "**Evaluation:** All signals tested against forward returns at 5m, 15m, 1h with a **minimum edge threshold of 15-20 bps** (to clear VIP0 fees + slippage).\n",
    "\n",
    "**Data:** BTCUSDT tick trades + OHLCV, 92 days, 6 sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9298a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "PARQUET_DIR = Path('../parquet')\n",
    "SYMBOL = 'BTCUSDT'\n",
    "\n",
    "FUTURES_SOURCES = ['binance_futures', 'bybit_futures', 'okx_futures']\n",
    "SOURCE_LABELS = {\n",
    "    'binance_futures': 'Binance', 'bybit_futures': 'Bybit', 'okx_futures': 'OKX',\n",
    "}\n",
    "SOURCE_COLORS = {\n",
    "    'binance_futures': '#F0B90B', 'bybit_futures': '#FF6B00', 'okx_futures': '#00C4B4',\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Loaders\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_trades_day(symbol, source, date):\n",
    "    \"\"\"Load trades for a single day.\"\"\"\n",
    "    path = PARQUET_DIR / symbol / 'trades' / source / f'{date}.parquet'\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame()\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def load_trades_range(symbol, source, start_date, end_date):\n",
    "    \"\"\"Load trades for a date range.\"\"\"\n",
    "    dates = pd.date_range(start_date, end_date)\n",
    "    dfs = []\n",
    "    for d in dates:\n",
    "        df = load_trades_day(symbol, source, d.strftime('%Y-%m-%d'))\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def load_ohlcv(symbol, interval, source):\n",
    "    \"\"\"Load all OHLCV for a source.\"\"\"\n",
    "    ohlcv_dir = PARQUET_DIR / symbol / 'ohlcv' / interval / source\n",
    "    if not ohlcv_dir.exists():\n",
    "        return pd.DataFrame()\n",
    "    files = sorted(ohlcv_dir.glob('*.parquet'))\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "    df = df.sort_values('timestamp_us').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "print(f'Ready. Symbol: {SYMBOL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac980e72",
   "metadata": {},
   "source": [
    "## 1. Build Microstructure Features from Tick Data\n",
    "\n",
    "For each 5-minute window, we compute features from raw trades that describe the *character* of activity inside the candle. We process day-by-day to keep memory bounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb48fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_microstructure_features(trades: pd.DataFrame, interval_us: int = 300_000_000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute microstructure features from raw tick trades, aggregated into fixed intervals.\n",
    "    \n",
    "    Args:\n",
    "        trades: DataFrame with columns [timestamp_us, price, quantity, quote_quantity, side]\n",
    "        interval_us: aggregation interval in microseconds (default 5 min = 300_000_000)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame indexed by interval start timestamp with microstructure features.\n",
    "    \"\"\"\n",
    "    ts = trades['timestamp_us'].values\n",
    "    price = trades['price'].values\n",
    "    qty = trades['quantity'].values\n",
    "    quote_qty = trades['quote_quantity'].values\n",
    "    side = trades['side'].values  # 1=buy, -1=sell\n",
    "    \n",
    "    # Assign each trade to an interval bucket\n",
    "    bucket = (ts // interval_us) * interval_us\n",
    "    trades = trades.copy()\n",
    "    trades['bucket'] = bucket\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for bkt, grp in trades.groupby('bucket'):\n",
    "        p = grp['price'].values\n",
    "        q = grp['quantity'].values\n",
    "        qq = grp['quote_quantity'].values\n",
    "        s = grp['side'].values\n",
    "        t = grp['timestamp_us'].values\n",
    "        n = len(grp)\n",
    "        \n",
    "        if n < 2:\n",
    "            continue\n",
    "        \n",
    "        buy_mask = s == 1\n",
    "        sell_mask = s == -1\n",
    "        buy_vol = q[buy_mask].sum()\n",
    "        sell_vol = q[sell_mask].sum()\n",
    "        total_vol = q.sum()\n",
    "        buy_quote = qq[buy_mask].sum()\n",
    "        sell_quote = qq[sell_mask].sum()\n",
    "        \n",
    "        # --- 1. AGGRESSION FEATURES ---\n",
    "        # Volume imbalance (normalized)\n",
    "        vol_imbalance = (buy_vol - sell_vol) / max(total_vol, 1e-10)\n",
    "        \n",
    "        # Quote-weighted imbalance ($ flow)\n",
    "        dollar_imbalance = (buy_quote - sell_quote) / max(buy_quote + sell_quote, 1e-10)\n",
    "        \n",
    "        # Large trade detection (trades > 90th percentile size)\n",
    "        q90 = np.percentile(q, 90)\n",
    "        large_mask = q >= q90\n",
    "        large_buy_vol = q[large_mask & buy_mask].sum()\n",
    "        large_sell_vol = q[large_mask & sell_mask].sum()\n",
    "        large_imbalance = (large_buy_vol - large_sell_vol) / max(large_buy_vol + large_sell_vol, 1e-10)\n",
    "        \n",
    "        # Aggression ratio: what fraction of volume is from large trades\n",
    "        large_vol_pct = q[large_mask].sum() / max(total_vol, 1e-10)\n",
    "        \n",
    "        # --- 2. FLOW PATTERN FEATURES ---\n",
    "        # Trade count\n",
    "        trade_count = n\n",
    "        buy_count = buy_mask.sum()\n",
    "        sell_count = sell_mask.sum()\n",
    "        count_imbalance = (buy_count - sell_count) / max(n, 1)\n",
    "        \n",
    "        # Trade arrival rate (trades per second)\n",
    "        duration_s = max((t[-1] - t[0]) / 1e6, 0.001)\n",
    "        arrival_rate = n / duration_s\n",
    "        \n",
    "        # Trade clustering: std of inter-trade times (lower = more clustered)\n",
    "        if n > 2:\n",
    "            iti = np.diff(t).astype(np.float64)\n",
    "            iti_mean = iti.mean()\n",
    "            iti_std = iti.std()\n",
    "            iti_cv = iti_std / max(iti_mean, 1)  # coefficient of variation\n",
    "            # Burstiness: fraction of trades in the most active 20% of the interval\n",
    "            sub_buckets = np.linspace(t[0], t[-1], 6)  # 5 sub-intervals\n",
    "            sub_counts = np.histogram(t, bins=sub_buckets)[0]\n",
    "            burstiness = sub_counts.max() / max(n, 1)\n",
    "        else:\n",
    "            iti_cv = 0\n",
    "            burstiness = 1.0\n",
    "        \n",
    "        # Acceleration: trade rate in 2nd half vs 1st half\n",
    "        mid_t = (t[0] + t[-1]) / 2\n",
    "        first_half = (t < mid_t).sum()\n",
    "        second_half = n - first_half\n",
    "        trade_acceleration = (second_half - first_half) / max(n, 1)\n",
    "        \n",
    "        # --- 3. PRICE IMPACT FEATURES ---\n",
    "        # VWAP\n",
    "        vwap = qq.sum() / max(total_vol, 1e-10)\n",
    "        \n",
    "        # Price range\n",
    "        price_range = (p.max() - p.min()) / max(vwap, 1e-10)  # normalized\n",
    "        \n",
    "        # Close vs VWAP (where did price end relative to average)\n",
    "        close_vs_vwap = (p[-1] - vwap) / max(vwap, 1e-10)\n",
    "        \n",
    "        # Kyle's lambda proxy: price impact per unit volume\n",
    "        # Regress price changes on signed volume\n",
    "        if n > 10:\n",
    "            signed_vol = q * s\n",
    "            price_changes = np.diff(p)\n",
    "            if len(price_changes) > 1 and signed_vol[1:].std() > 0:\n",
    "                kyle_lambda = np.corrcoef(signed_vol[1:], price_changes)[0, 1]\n",
    "            else:\n",
    "                kyle_lambda = 0\n",
    "        else:\n",
    "            kyle_lambda = 0\n",
    "        \n",
    "        # Amihud illiquidity: |return| / volume\n",
    "        ret = (p[-1] - p[0]) / max(p[0], 1e-10)\n",
    "        amihud = abs(ret) / max(total_vol, 1e-10)\n",
    "        \n",
    "        # --- 4. VOLUME PROFILE FEATURES ---\n",
    "        # Volume at high vs low: where in the price range did most volume trade?\n",
    "        price_mid = (p.max() + p.min()) / 2\n",
    "        vol_above_mid = q[p >= price_mid].sum()\n",
    "        vol_below_mid = q[p < price_mid].sum()\n",
    "        vol_profile_skew = (vol_above_mid - vol_below_mid) / max(total_vol, 1e-10)\n",
    "        \n",
    "        # Volume-weighted price std (dispersion)\n",
    "        if total_vol > 0:\n",
    "            vol_weighted_std = np.sqrt(np.average((p - vwap)**2, weights=q))\n",
    "        else:\n",
    "            vol_weighted_std = 0\n",
    "        \n",
    "        # --- 5. OHLCV-DERIVED ---\n",
    "        open_price = p[0]\n",
    "        close_price = p[-1]\n",
    "        high_price = p.max()\n",
    "        low_price = p.min()\n",
    "        \n",
    "        # Upper/lower wick ratios\n",
    "        body = abs(close_price - open_price)\n",
    "        full_range = high_price - low_price\n",
    "        if full_range > 0:\n",
    "            upper_wick = (high_price - max(open_price, close_price)) / full_range\n",
    "            lower_wick = (min(open_price, close_price) - low_price) / full_range\n",
    "        else:\n",
    "            upper_wick = 0\n",
    "            lower_wick = 0\n",
    "        \n",
    "        features.append({\n",
    "            'timestamp_us': bkt,\n",
    "            # Aggression\n",
    "            'vol_imbalance': vol_imbalance,\n",
    "            'dollar_imbalance': dollar_imbalance,\n",
    "            'large_imbalance': large_imbalance,\n",
    "            'large_vol_pct': large_vol_pct,\n",
    "            # Flow\n",
    "            'trade_count': trade_count,\n",
    "            'count_imbalance': count_imbalance,\n",
    "            'arrival_rate': arrival_rate,\n",
    "            'iti_cv': iti_cv,\n",
    "            'burstiness': burstiness,\n",
    "            'trade_acceleration': trade_acceleration,\n",
    "            # Price impact\n",
    "            'vwap': vwap,\n",
    "            'price_range': price_range,\n",
    "            'close_vs_vwap': close_vs_vwap,\n",
    "            'kyle_lambda': kyle_lambda,\n",
    "            'amihud': amihud,\n",
    "            # Volume profile\n",
    "            'vol_profile_skew': vol_profile_skew,\n",
    "            'vol_weighted_std': vol_weighted_std,\n",
    "            # Candle shape\n",
    "            'upper_wick': upper_wick,\n",
    "            'lower_wick': lower_wick,\n",
    "            # Raw\n",
    "            'open': open_price,\n",
    "            'close': close_price,\n",
    "            'high': high_price,\n",
    "            'low': low_price,\n",
    "            'volume': total_vol,\n",
    "            'buy_volume': buy_vol,\n",
    "            'sell_volume': sell_vol,\n",
    "            'quote_volume': buy_quote + sell_quote,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "print(f'Feature builder ready. {len(\"compute_microstructure_features\")} features per bar.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94035cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build features for Binance Futures, processing day-by-day to keep memory bounded\n",
    "# Using 5-minute intervals\n",
    "\n",
    "source = 'binance_futures'\n",
    "INTERVAL_US = 300_000_000  # 5 min\n",
    "\n",
    "dates = sorted([f.stem for f in (PARQUET_DIR / SYMBOL / 'trades' / source).glob('*.parquet')])\n",
    "print(f'Processing {len(dates)} days for {source}...')\n",
    "\n",
    "all_features = []\n",
    "for i, date in enumerate(dates):\n",
    "    trades = load_trades_day(SYMBOL, source, date)\n",
    "    if trades.empty:\n",
    "        continue\n",
    "    feat = compute_microstructure_features(trades, INTERVAL_US)\n",
    "    all_features.append(feat)\n",
    "    if (i + 1) % 10 == 0 or i == len(dates) - 1:\n",
    "        print(f'  [{i+1}/{len(dates)}] {date}  bars={len(feat)}')\n",
    "    del trades\n",
    "\n",
    "features_bn = pd.concat(all_features, ignore_index=True).sort_values('timestamp_us').reset_index(drop=True)\n",
    "features_bn['datetime'] = pd.to_datetime(features_bn['timestamp_us'], unit='us', utc=True)\n",
    "features_bn['returns'] = features_bn['close'].pct_change()\n",
    "\n",
    "print(f'\\nTotal: {len(features_bn):,} bars, {len(features_bn.columns)} columns')\n",
    "print(f'Date range: {features_bn[\"datetime\"].min()} → {features_bn[\"datetime\"].max()}')\n",
    "features_bn.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73883a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build features for Bybit and OKX Futures too\n",
    "features_all = {'binance_futures': features_bn}\n",
    "\n",
    "for source in ['bybit_futures', 'okx_futures']:\n",
    "    dates = sorted([f.stem for f in (PARQUET_DIR / SYMBOL / 'trades' / source).glob('*.parquet')])\n",
    "    print(f'Processing {len(dates)} days for {source}...')\n",
    "    \n",
    "    all_feat = []\n",
    "    for i, date in enumerate(dates):\n",
    "        trades = load_trades_day(SYMBOL, source, date)\n",
    "        if trades.empty:\n",
    "            continue\n",
    "        feat = compute_microstructure_features(trades, INTERVAL_US)\n",
    "        all_feat.append(feat)\n",
    "        if (i + 1) % 30 == 0 or i == len(dates) - 1:\n",
    "            print(f'  [{i+1}/{len(dates)}] {date}')\n",
    "        del trades\n",
    "    \n",
    "    df = pd.concat(all_feat, ignore_index=True).sort_values('timestamp_us').reset_index(drop=True)\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp_us'], unit='us', utc=True)\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    features_all[source] = df\n",
    "    print(f'  → {len(df):,} bars\\n')\n",
    "\n",
    "print('All sources processed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc704d8",
   "metadata": {},
   "source": [
    "## 2. Feature Overview & Distributions\n",
    "\n",
    "Quick sanity check on feature distributions before signal evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde66e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features_bn  # Use Binance as primary for analysis\n",
    "\n",
    "feature_cols = [\n",
    "    'vol_imbalance', 'dollar_imbalance', 'large_imbalance', 'large_vol_pct',\n",
    "    'count_imbalance', 'arrival_rate', 'iti_cv', 'burstiness', 'trade_acceleration',\n",
    "    'price_range', 'close_vs_vwap', 'kyle_lambda', 'amihud',\n",
    "    'vol_profile_skew', 'vol_weighted_std', 'upper_wick', 'lower_wick',\n",
    "]\n",
    "\n",
    "print(f'Binance Futures — Feature Summary ({len(df):,} bars)')\n",
    "print(f'{\"─\" * 90}')\n",
    "print(f'{\"Feature\":25s} {\"Mean\":>10s} {\"Std\":>10s} {\"Min\":>10s} {\"P5\":>10s} {\"P95\":>10s} {\"Max\":>10s}')\n",
    "print(f'{\"─\" * 90}')\n",
    "for col in feature_cols:\n",
    "    s = df[col]\n",
    "    print(f'{col:25s} {s.mean():>10.4f} {s.std():>10.4f} {s.min():>10.4f} '\n",
    "          f'{s.quantile(0.05):>10.4f} {s.quantile(0.95):>10.4f} {s.max():>10.4f}')\n",
    "\n",
    "# Plot key feature distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "fig.suptitle(f'{SYMBOL} — Microstructure Feature Distributions (Binance Futures, 5m)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plot_features = ['vol_imbalance', 'dollar_imbalance', 'large_imbalance',\n",
    "                 'trade_acceleration', 'kyle_lambda', 'close_vs_vwap',\n",
    "                 'burstiness', 'arrival_rate', 'price_range']\n",
    "\n",
    "for ax, feat in zip(axes.flat, plot_features):\n",
    "    vals = df[feat].dropna()\n",
    "    # Clip for visualization\n",
    "    lo, hi = vals.quantile(0.01), vals.quantile(0.99)\n",
    "    ax.hist(vals.clip(lo, hi), bins=100, alpha=0.7, color='steelblue', density=True)\n",
    "    ax.set_title(feat)\n",
    "    ax.axvline(0, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcbf0f",
   "metadata": {},
   "source": [
    "## 3. Signal Evaluation: Feature → Forward Return Predictiveness\n",
    "\n",
    "For each feature, compute:\n",
    "- **IC (Information Coefficient)**: rank correlation with forward returns at 5m, 15m, 1h\n",
    "- **Decile spread**: mean return of top decile minus bottom decile\n",
    "- **IC stability**: rolling IC to check if signal is consistent over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bebc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_forward_returns(df):\n",
    "    \"\"\"Add forward returns at multiple horizons.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['fwd_1bar'] = df['close'].pct_change(1).shift(-1)    # 5m forward\n",
    "    df['fwd_3bar'] = df['close'].pct_change(3).shift(-3)    # 15m forward\n",
    "    df['fwd_12bar'] = df['close'].pct_change(12).shift(-12)  # 1h forward\n",
    "    return df\n",
    "\n",
    "def compute_ic_table(df, feature_cols, fwd_cols=['fwd_1bar', 'fwd_3bar', 'fwd_12bar']):\n",
    "    \"\"\"Compute rank IC (Spearman) for each feature vs each forward return horizon.\"\"\"\n",
    "    results = []\n",
    "    for feat in feature_cols:\n",
    "        row = {'feature': feat}\n",
    "        for fwd in fwd_cols:\n",
    "            clean = df[[feat, fwd]].dropna()\n",
    "            if len(clean) < 100:\n",
    "                row[fwd] = np.nan\n",
    "                continue\n",
    "            ic, pval = stats.spearmanr(clean[feat], clean[fwd])\n",
    "            row[fwd] = ic\n",
    "            row[f'{fwd}_pval'] = pval\n",
    "        results.append(row)\n",
    "    return pd.DataFrame(results).set_index('feature')\n",
    "\n",
    "def compute_decile_spread(df, feature_cols, fwd_col='fwd_3bar'):\n",
    "    \"\"\"Compute mean forward return by decile for each feature.\"\"\"\n",
    "    results = []\n",
    "    for feat in feature_cols:\n",
    "        clean = df[[feat, fwd_col]].dropna()\n",
    "        if len(clean) < 100:\n",
    "            continue\n",
    "        clean['decile'] = pd.qcut(clean[feat], q=10, labels=False, duplicates='drop')\n",
    "        decile_ret = clean.groupby('decile')[fwd_col].mean()\n",
    "        spread = (decile_ret.iloc[-1] - decile_ret.iloc[0]) * 10000  # bps\n",
    "        results.append({'feature': feat, 'D10_minus_D1_bps': spread,\n",
    "                       'D1_bps': decile_ret.iloc[0] * 10000,\n",
    "                       'D10_bps': decile_ret.iloc[-1] * 10000})\n",
    "    return pd.DataFrame(results).set_index('feature')\n",
    "\n",
    "# Add forward returns to Binance features\n",
    "df = add_forward_returns(features_bn)\n",
    "\n",
    "# Compute IC table\n",
    "ic_table = compute_ic_table(df, feature_cols)\n",
    "print(f'{SYMBOL} — Information Coefficient (Rank IC) vs Forward Returns')\n",
    "print(f'{\"─\" * 80}')\n",
    "print(f'{\"Feature\":25s} {\"IC 5m\":>10s} {\"IC 15m\":>10s} {\"IC 1h\":>10s}  {\"Significant?\":>12s}')\n",
    "print(f'{\"─\" * 80}')\n",
    "for feat in feature_cols:\n",
    "    ic5 = ic_table.loc[feat, 'fwd_1bar']\n",
    "    ic15 = ic_table.loc[feat, 'fwd_3bar']\n",
    "    ic1h = ic_table.loc[feat, 'fwd_12bar']\n",
    "    # Mark significant if |IC| > 0.02 and p < 0.01\n",
    "    sig = ''\n",
    "    for fwd in ['fwd_1bar', 'fwd_3bar', 'fwd_12bar']:\n",
    "        pval_col = f'{fwd}_pval'\n",
    "        if pval_col in ic_table.columns:\n",
    "            if abs(ic_table.loc[feat, fwd]) > 0.02 and ic_table.loc[feat, pval_col] < 0.01:\n",
    "                sig = '***'\n",
    "                break\n",
    "            elif abs(ic_table.loc[feat, fwd]) > 0.01 and ic_table.loc[feat, pval_col] < 0.01:\n",
    "                sig = '**'\n",
    "    print(f'{feat:25s} {ic5:>10.4f} {ic15:>10.4f} {ic1h:>10.4f}  {sig:>12s}')\n",
    "\n",
    "# Highlight top signals\n",
    "print(f'\\n{\"=\" * 60}')\n",
    "print('Top signals by |IC| at 15m horizon:')\n",
    "top = ic_table['fwd_3bar'].abs().sort_values(ascending=False).head(8)\n",
    "for feat, ic in top.items():\n",
    "    print(f'  {feat:25s}  IC={ic_table.loc[feat, \"fwd_3bar\"]:+.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decile spread analysis for 15m forward returns\n",
    "spread_table = compute_decile_spread(df, feature_cols, 'fwd_3bar')\n",
    "\n",
    "print(f'{SYMBOL} — Decile Spread (15m forward return, bps)')\n",
    "print(f'{\"─\" * 70}')\n",
    "print(f'{\"Feature\":25s} {\"D1 (low)\":>10s} {\"D10 (high)\":>10s} {\"Spread\":>10s}  {\"Tradeable?\":>10s}')\n",
    "print(f'{\"─\" * 70}')\n",
    "for feat in spread_table.index:\n",
    "    d1 = spread_table.loc[feat, 'D1_bps']\n",
    "    d10 = spread_table.loc[feat, 'D10_bps']\n",
    "    spread = spread_table.loc[feat, 'D10_minus_D1_bps']\n",
    "    tradeable = '✓' if abs(spread) > 15 else ''\n",
    "    print(f'{feat:25s} {d1:>10.2f} {d10:>10.2f} {spread:>10.2f}  {tradeable:>10s}')\n",
    "\n",
    "# Plot decile returns for top features\n",
    "top_feats = spread_table['D10_minus_D1_bps'].abs().sort_values(ascending=False).head(6).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 9))\n",
    "fig.suptitle(f'{SYMBOL} — 15m Forward Return by Feature Decile (Binance Futures)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax, feat in zip(axes.flat, top_feats):\n",
    "    clean = df[[feat, 'fwd_3bar']].dropna().copy()\n",
    "    clean['decile'] = pd.qcut(clean[feat], q=10, labels=False, duplicates='drop')\n",
    "    decile_ret = clean.groupby('decile')['fwd_3bar'].mean() * 10000\n",
    "    colors = ['#d32f2f' if v < 0 else '#388e3c' for v in decile_ret.values]\n",
    "    ax.bar(decile_ret.index, decile_ret.values, color=colors, alpha=0.7)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.set_title(f'{feat}\\nspread={spread_table.loc[feat, \"D10_minus_D1_bps\"]:.1f} bps')\n",
    "    ax.set_xlabel('Decile (0=low, 9=high)')\n",
    "    ax.set_ylabel('Mean 15m Fwd Return (bps)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6d935",
   "metadata": {},
   "source": [
    "## 4. IC Stability Over Time\n",
    "\n",
    "A signal is only useful if it's consistent. Check rolling IC to see if top features maintain predictiveness across the 3-month period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling IC for top features (weekly windows)\n",
    "top_feats_for_stability = spread_table['D10_minus_D1_bps'].abs().sort_values(ascending=False).head(6).index.tolist()\n",
    "\n",
    "df['date'] = df['datetime'].dt.date\n",
    "weekly_groups = df.groupby(pd.Grouper(key='datetime', freq='W'))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "fig.suptitle(f'{SYMBOL} — Rolling Weekly IC (feature vs 15m fwd return)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax, feat in zip(axes.flat, top_feats_for_stability):\n",
    "    weekly_ics = []\n",
    "    weekly_dates = []\n",
    "    for week_start, grp in weekly_groups:\n",
    "        clean = grp[[feat, 'fwd_3bar']].dropna()\n",
    "        if len(clean) < 50:\n",
    "            continue\n",
    "        ic, _ = stats.spearmanr(clean[feat], clean['fwd_3bar'])\n",
    "        weekly_ics.append(ic)\n",
    "        weekly_dates.append(week_start)\n",
    "    \n",
    "    if not weekly_ics:\n",
    "        continue\n",
    "    \n",
    "    colors = ['#388e3c' if v > 0 else '#d32f2f' for v in weekly_ics]\n",
    "    ax.bar(weekly_dates, weekly_ics, width=5, color=colors, alpha=0.7)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    mean_ic = np.mean(weekly_ics)\n",
    "    hit_rate = np.mean([1 if ic * mean_ic > 0 else 0 for ic in weekly_ics])\n",
    "    ax.set_title(f'{feat}\\nmean IC={mean_ic:+.4f}, consistency={hit_rate:.0%}')\n",
    "    ax.set_ylabel('Weekly IC')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c0ae4",
   "metadata": {},
   "source": [
    "## 5. Cross-Exchange Feature Comparison\n",
    "\n",
    "Do the same features have predictive power across all 3 exchanges? If so, the signal is more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare IC across all 3 exchanges\n",
    "print(f'{SYMBOL} — Cross-Exchange IC Comparison (15m forward return)')\n",
    "print(f'{\"─\" * 75}')\n",
    "print(f'{\"Feature\":25s} {\"Binance\":>12s} {\"Bybit\":>12s} {\"OKX\":>12s} {\"Consistent?\":>12s}')\n",
    "print(f'{\"─\" * 75}')\n",
    "\n",
    "cross_ic = {}\n",
    "for source in FUTURES_SOURCES:\n",
    "    df_src = add_forward_returns(features_all[source])\n",
    "    ic_src = compute_ic_table(df_src, feature_cols, ['fwd_3bar'])\n",
    "    cross_ic[source] = ic_src['fwd_3bar']\n",
    "\n",
    "for feat in feature_cols:\n",
    "    ics = [cross_ic[src].get(feat, np.nan) for src in FUTURES_SOURCES]\n",
    "    # Consistent if all same sign and all |IC| > 0.005\n",
    "    same_sign = all(ic > 0 for ic in ics if not np.isnan(ic)) or all(ic < 0 for ic in ics if not np.isnan(ic))\n",
    "    all_meaningful = all(abs(ic) > 0.005 for ic in ics if not np.isnan(ic))\n",
    "    consistent = '✓' if same_sign and all_meaningful else ''\n",
    "    print(f'{feat:25s} {ics[0]:>12.4f} {ics[1]:>12.4f} {ics[2]:>12.4f} {consistent:>12s}')\n",
    "\n",
    "# Highlight features consistent across all exchanges\n",
    "print(f'\\n{\"=\" * 60}')\n",
    "print('Features with consistent signal across all 3 exchanges:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a42c7",
   "metadata": {},
   "source": [
    "## 6. Cross-Exchange Volume Divergence\n",
    "\n",
    "When one exchange sees unusual volume relative to others, it may signal informed flow. Build features that compare activity across venues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b540d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge features across exchanges on timestamp to build cross-exchange divergence features\n",
    "bn = features_all['binance_futures'][['timestamp_us', 'vol_imbalance', 'dollar_imbalance',\n",
    "    'large_imbalance', 'arrival_rate', 'trade_acceleration', 'volume', 'buy_volume',\n",
    "    'sell_volume', 'close', 'kyle_lambda']].copy()\n",
    "bb = features_all['bybit_futures'][['timestamp_us', 'vol_imbalance', 'dollar_imbalance',\n",
    "    'large_imbalance', 'arrival_rate', 'trade_acceleration', 'volume', 'buy_volume',\n",
    "    'sell_volume', 'close', 'kyle_lambda']].copy()\n",
    "okx = features_all['okx_futures'][['timestamp_us', 'vol_imbalance', 'dollar_imbalance',\n",
    "    'large_imbalance', 'arrival_rate', 'trade_acceleration', 'volume', 'buy_volume',\n",
    "    'sell_volume', 'close', 'kyle_lambda']].copy()\n",
    "\n",
    "# Merge all three\n",
    "cross = bn.merge(bb, on='timestamp_us', suffixes=('_bn', '_bb'))\n",
    "cross = cross.merge(okx, on='timestamp_us')\n",
    "# Rename OKX columns\n",
    "for col in ['vol_imbalance', 'dollar_imbalance', 'large_imbalance', 'arrival_rate',\n",
    "            'trade_acceleration', 'volume', 'buy_volume', 'sell_volume', 'close', 'kyle_lambda']:\n",
    "    cross.rename(columns={col: f'{col}_okx'}, inplace=True)\n",
    "\n",
    "cross['datetime'] = pd.to_datetime(cross['timestamp_us'], unit='us', utc=True)\n",
    "\n",
    "# --- Cross-exchange divergence features ---\n",
    "# 1. Volume share divergence: is one exchange getting unusual share of total volume?\n",
    "total_vol = cross['volume_bn'] + cross['volume_bb'] + cross['volume_okx']\n",
    "cross['vol_share_bn'] = cross['volume_bn'] / total_vol\n",
    "cross['vol_share_bb'] = cross['volume_bb'] / total_vol\n",
    "cross['vol_share_okx'] = cross['volume_okx'] / total_vol\n",
    "\n",
    "# Rolling z-score of volume share (unusual activity)\n",
    "for exch in ['bn', 'bb', 'okx']:\n",
    "    col = f'vol_share_{exch}'\n",
    "    cross[f'vol_share_zscore_{exch}'] = (\n",
    "        (cross[col] - cross[col].rolling(288).mean()) / cross[col].rolling(288).std()\n",
    "    )  # 288 bars = 1 day of 5m bars\n",
    "\n",
    "# 2. Imbalance divergence: when exchanges disagree on buy/sell pressure\n",
    "cross['imbalance_consensus'] = (\n",
    "    cross['vol_imbalance_bn'] + cross['vol_imbalance_bb'] + cross['vol_imbalance_okx']\n",
    ") / 3\n",
    "cross['imbalance_divergence'] = (\n",
    "    (cross['vol_imbalance_bn'] - cross['imbalance_consensus']).abs() +\n",
    "    (cross['vol_imbalance_bb'] - cross['imbalance_consensus']).abs() +\n",
    "    (cross['vol_imbalance_okx'] - cross['imbalance_consensus']).abs()\n",
    ") / 3\n",
    "\n",
    "# 3. Large trade imbalance divergence\n",
    "cross['large_imb_consensus'] = (\n",
    "    cross['large_imbalance_bn'] + cross['large_imbalance_bb'] + cross['large_imbalance_okx']\n",
    ") / 3\n",
    "\n",
    "# 4. Binance-led imbalance: Binance imbalance minus average of others\n",
    "cross['bn_imb_lead'] = cross['vol_imbalance_bn'] - (cross['vol_imbalance_bb'] + cross['vol_imbalance_okx']) / 2\n",
    "\n",
    "# 5. Arrival rate divergence\n",
    "total_rate = cross['arrival_rate_bn'] + cross['arrival_rate_bb'] + cross['arrival_rate_okx']\n",
    "cross['rate_share_bn'] = cross['arrival_rate_bn'] / total_rate\n",
    "\n",
    "# Forward returns (use Binance close as reference)\n",
    "cross['fwd_1bar'] = cross['close_bn'].pct_change(1).shift(-1)\n",
    "cross['fwd_3bar'] = cross['close_bn'].pct_change(3).shift(-3)\n",
    "cross['fwd_12bar'] = cross['close_bn'].pct_change(12).shift(-12)\n",
    "\n",
    "print(f'Cross-exchange dataset: {len(cross):,} matched 5m bars')\n",
    "print(f'New features: vol_share_zscore, imbalance_consensus, imbalance_divergence, bn_imb_lead, etc.')\n",
    "\n",
    "# Evaluate cross-exchange features\n",
    "cross_features = [\n",
    "    'vol_share_zscore_bn', 'vol_share_zscore_bb', 'vol_share_zscore_okx',\n",
    "    'imbalance_consensus', 'imbalance_divergence',\n",
    "    'large_imb_consensus', 'bn_imb_lead', 'rate_share_bn',\n",
    "]\n",
    "\n",
    "ic_cross = compute_ic_table(cross, cross_features, ['fwd_1bar', 'fwd_3bar', 'fwd_12bar'])\n",
    "spread_cross = compute_decile_spread(cross, cross_features, 'fwd_3bar')\n",
    "\n",
    "print(f'\\n{SYMBOL} — Cross-Exchange Feature IC (15m forward)')\n",
    "print(f'{\"─\" * 70}')\n",
    "print(f'{\"Feature\":30s} {\"IC 5m\":>10s} {\"IC 15m\":>10s} {\"IC 1h\":>10s} {\"Spread bps\":>12s}')\n",
    "print(f'{\"─\" * 70}')\n",
    "for feat in cross_features:\n",
    "    ic5 = ic_cross.loc[feat, 'fwd_1bar']\n",
    "    ic15 = ic_cross.loc[feat, 'fwd_3bar']\n",
    "    ic1h = ic_cross.loc[feat, 'fwd_12bar']\n",
    "    sp = spread_cross.loc[feat, 'D10_minus_D1_bps'] if feat in spread_cross.index else np.nan\n",
    "    print(f'{feat:30s} {ic5:>10.4f} {ic15:>10.4f} {ic1h:>10.4f} {sp:>12.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7bea1",
   "metadata": {},
   "source": [
    "## 7. Volatility Regime Features\n",
    "\n",
    "Volatility clustering was the strongest signal in profiling. Build regime features and test if they improve signal quality when combined with microstructure features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build volatility regime features on Binance 5m data\n",
    "df = features_bn.copy()\n",
    "\n",
    "# Realized volatility (rolling std of returns)\n",
    "df['returns'] = df['close'].pct_change()\n",
    "df['rvol_12'] = df['returns'].rolling(12).std()   # 1h realized vol\n",
    "df['rvol_60'] = df['returns'].rolling(60).std()    # 5h realized vol\n",
    "df['rvol_288'] = df['returns'].rolling(288).std()  # 1d realized vol\n",
    "\n",
    "# Vol ratio: short-term vs long-term (vol expansion/compression)\n",
    "df['vol_ratio_12_60'] = df['rvol_12'] / df['rvol_60'].clip(lower=1e-10)\n",
    "df['vol_ratio_12_288'] = df['rvol_12'] / df['rvol_288'].clip(lower=1e-10)\n",
    "\n",
    "# Vol z-score: how unusual is current vol vs recent history\n",
    "df['vol_zscore'] = (df['rvol_12'] - df['rvol_288']) / df['rvol_288'].rolling(288).std().clip(lower=1e-10)\n",
    "\n",
    "# Vol acceleration: change in vol\n",
    "df['vol_accel'] = df['rvol_12'].pct_change(6)  # 30m change in 1h vol\n",
    "\n",
    "# Price range relative to vol (normalized range)\n",
    "df['range_vs_vol'] = df['price_range'] / df['rvol_12'].clip(lower=1e-10)\n",
    "\n",
    "# Combine vol regime with directional features\n",
    "df['imb_x_vol'] = df['vol_imbalance'] * df['vol_ratio_12_288']  # imbalance amplified by vol expansion\n",
    "df['large_imb_x_vol'] = df['large_imbalance'] * df['vol_ratio_12_288']\n",
    "df['kyle_x_vol'] = df['kyle_lambda'] * df['vol_ratio_12_288']\n",
    "\n",
    "# Forward returns\n",
    "df['fwd_1bar'] = df['close'].pct_change(1).shift(-1)\n",
    "df['fwd_3bar'] = df['close'].pct_change(3).shift(-3)\n",
    "df['fwd_12bar'] = df['close'].pct_change(12).shift(-12)\n",
    "\n",
    "# Evaluate vol features\n",
    "vol_features = [\n",
    "    'rvol_12', 'rvol_60', 'vol_ratio_12_60', 'vol_ratio_12_288',\n",
    "    'vol_zscore', 'vol_accel', 'range_vs_vol',\n",
    "    'imb_x_vol', 'large_imb_x_vol', 'kyle_x_vol',\n",
    "]\n",
    "\n",
    "ic_vol = compute_ic_table(df.dropna(), vol_features, ['fwd_1bar', 'fwd_3bar', 'fwd_12bar'])\n",
    "spread_vol = compute_decile_spread(df.dropna(), vol_features, 'fwd_3bar')\n",
    "\n",
    "print(f'{SYMBOL} — Volatility & Interaction Feature IC')\n",
    "print(f'{\"─\" * 80}')\n",
    "print(f'{\"Feature\":25s} {\"IC 5m\":>10s} {\"IC 15m\":>10s} {\"IC 1h\":>10s} {\"Spread bps\":>12s}')\n",
    "print(f'{\"─\" * 80}')\n",
    "for feat in vol_features:\n",
    "    ic5 = ic_vol.loc[feat, 'fwd_1bar']\n",
    "    ic15 = ic_vol.loc[feat, 'fwd_3bar']\n",
    "    ic1h = ic_vol.loc[feat, 'fwd_12bar']\n",
    "    sp = spread_vol.loc[feat, 'D10_minus_D1_bps'] if feat in spread_vol.index else np.nan\n",
    "    marker = ' ✓' if abs(sp) > 15 else ''\n",
    "    print(f'{feat:25s} {ic5:>10.4f} {ic15:>10.4f} {ic1h:>10.4f} {sp:>12.1f}{marker}')\n",
    "\n",
    "# Plot interaction features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f'{SYMBOL} — Vol-Interaction Features: 15m Fwd Return by Decile', fontsize=14, fontweight='bold')\n",
    "\n",
    "for ax, feat in zip(axes.flat, ['imb_x_vol', 'large_imb_x_vol', 'kyle_x_vol']):\n",
    "    clean = df[[feat, 'fwd_3bar']].dropna().copy()\n",
    "    clean['decile'] = pd.qcut(clean[feat], q=10, labels=False, duplicates='drop')\n",
    "    decile_ret = clean.groupby('decile')['fwd_3bar'].mean() * 10000\n",
    "    colors = ['#d32f2f' if v < 0 else '#388e3c' for v in decile_ret.values]\n",
    "    ax.bar(decile_ret.index, decile_ret.values, color=colors, alpha=0.7)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    sp = spread_vol.loc[feat, 'D10_minus_D1_bps'] if feat in spread_vol.index else 0\n",
    "    ax.set_title(f'{feat}\\nspread={sp:.1f} bps')\n",
    "    ax.set_xlabel('Decile')\n",
    "    ax.set_ylabel('Mean 15m Fwd Return (bps)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c9c5e",
   "metadata": {},
   "source": [
    "## 8. Conditional Analysis: Signals in Different Vol Regimes\n",
    "\n",
    "Do microstructure features work better in high-vol or low-vol environments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into vol regimes and compare feature IC\n",
    "df_clean = df.dropna(subset=['vol_ratio_12_288', 'fwd_3bar']).copy()\n",
    "\n",
    "# Define regimes: low vol (bottom 30%), normal (middle 40%), high vol (top 30%)\n",
    "df_clean['vol_regime'] = pd.qcut(df_clean['vol_ratio_12_288'], q=[0, 0.3, 0.7, 1.0],\n",
    "                                  labels=['low_vol', 'normal', 'high_vol'])\n",
    "\n",
    "key_features = ['vol_imbalance', 'dollar_imbalance', 'large_imbalance',\n",
    "                'kyle_lambda', 'close_vs_vwap', 'trade_acceleration',\n",
    "                'count_imbalance', 'burstiness']\n",
    "\n",
    "print(f'{SYMBOL} — IC by Volatility Regime (15m forward return)')\n",
    "print(f'{\"─\" * 80}')\n",
    "print(f'{\"Feature\":25s} {\"Low Vol\":>12s} {\"Normal\":>12s} {\"High Vol\":>12s} {\"Best Regime\":>12s}')\n",
    "print(f'{\"─\" * 80}')\n",
    "\n",
    "regime_ics = {}\n",
    "for feat in key_features:\n",
    "    row = {}\n",
    "    for regime in ['low_vol', 'normal', 'high_vol']:\n",
    "        subset = df_clean[df_clean['vol_regime'] == regime]\n",
    "        clean = subset[[feat, 'fwd_3bar']].dropna()\n",
    "        if len(clean) < 100:\n",
    "            row[regime] = np.nan\n",
    "            continue\n",
    "        ic, _ = stats.spearmanr(clean[feat], clean['fwd_3bar'])\n",
    "        row[regime] = ic\n",
    "    regime_ics[feat] = row\n",
    "    \n",
    "    best = max(row, key=lambda k: abs(row.get(k, 0) or 0))\n",
    "    print(f'{feat:25s} {row.get(\"low_vol\", 0):>12.4f} {row.get(\"normal\", 0):>12.4f} '\n",
    "          f'{row.get(\"high_vol\", 0):>12.4f} {best:>12s}')\n",
    "\n",
    "# Plot: decile spread in high vol vs low vol for top features\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 9))\n",
    "fig.suptitle(f'{SYMBOL} — Feature Decile Returns: High Vol vs Low Vol (15m fwd)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, feat in enumerate(key_features):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    for regime, color, ls in [('high_vol', '#d32f2f', '-'), ('low_vol', '#1565c0', '--')]:\n",
    "        subset = df_clean[df_clean['vol_regime'] == regime].copy()\n",
    "        clean = subset[[feat, 'fwd_3bar']].dropna().copy()\n",
    "        if len(clean) < 100:\n",
    "            continue\n",
    "        clean['decile'] = pd.qcut(clean[feat], q=10, labels=False, duplicates='drop')\n",
    "        decile_ret = clean.groupby('decile')['fwd_3bar'].mean() * 10000\n",
    "        ax.plot(decile_ret.index, decile_ret.values, marker='o', markersize=4,\n",
    "               color=color, linestyle=ls, label=regime, alpha=0.8)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.set_title(feat, fontsize=10)\n",
    "    ax.set_xlabel('Decile')\n",
    "    ax.set_ylabel('bps')\n",
    "    ax.legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e969abda",
   "metadata": {},
   "source": [
    "## 9. Summary & Feature Ranking\n",
    "\n",
    "Consolidate all findings: rank features by IC, decile spread, consistency, and regime robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42092db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final feature ranking: combine IC, spread, consistency, and cross-exchange agreement\n",
    "\n",
    "all_features_ranked = feature_cols + ['imb_x_vol', 'large_imb_x_vol', 'kyle_x_vol']\n",
    "\n",
    "# Recompute everything on the full df with vol features\n",
    "df_full = df.dropna(subset=['fwd_3bar']).copy()\n",
    "\n",
    "print(f'{SYMBOL} — FINAL FEATURE RANKING')\n",
    "print(f'{\"=\" * 100}')\n",
    "print(f'{\"Feature\":25s} {\"IC 5m\":>8s} {\"IC 15m\":>8s} {\"IC 1h\":>8s} '\n",
    "      f'{\"Spread\":>8s} {\"Consistent\":>10s} {\"Verdict\":>10s}')\n",
    "print(f'{\"─\" * 100}')\n",
    "\n",
    "ic_final = compute_ic_table(df_full, all_features_ranked, ['fwd_1bar', 'fwd_3bar', 'fwd_12bar'])\n",
    "spread_final = compute_decile_spread(df_full, all_features_ranked, 'fwd_3bar')\n",
    "\n",
    "verdicts = []\n",
    "for feat in all_features_ranked:\n",
    "    ic5 = ic_final.loc[feat, 'fwd_1bar'] if feat in ic_final.index else 0\n",
    "    ic15 = ic_final.loc[feat, 'fwd_3bar'] if feat in ic_final.index else 0\n",
    "    ic1h = ic_final.loc[feat, 'fwd_12bar'] if feat in ic_final.index else 0\n",
    "    sp = spread_final.loc[feat, 'D10_minus_D1_bps'] if feat in spread_final.index else 0\n",
    "    \n",
    "    # Check cross-exchange consistency (for base features only)\n",
    "    consistent = '—'\n",
    "    if feat in feature_cols:\n",
    "        ics_cross = []\n",
    "        for src in FUTURES_SOURCES:\n",
    "            if feat in cross_ic.get(src, pd.Series()).index:\n",
    "                ics_cross.append(cross_ic[src][feat])\n",
    "        if len(ics_cross) == 3:\n",
    "            same_sign = all(x > 0 for x in ics_cross) or all(x < 0 for x in ics_cross)\n",
    "            consistent = '✓' if same_sign and all(abs(x) > 0.005 for x in ics_cross) else '✗'\n",
    "    \n",
    "    # Verdict\n",
    "    if abs(sp) > 15 and abs(ic15) > 0.02:\n",
    "        verdict = '★★★'\n",
    "    elif abs(sp) > 10 and abs(ic15) > 0.015:\n",
    "        verdict = '★★'\n",
    "    elif abs(sp) > 5 and abs(ic15) > 0.01:\n",
    "        verdict = '★'\n",
    "    else:\n",
    "        verdict = ''\n",
    "    \n",
    "    verdicts.append((feat, abs(ic15), sp, verdict))\n",
    "    print(f'{feat:25s} {ic5:>8.4f} {ic15:>8.4f} {ic1h:>8.4f} {sp:>7.1f}bp {consistent:>10s} {verdict:>10s}')\n",
    "\n",
    "# Sort by |IC 15m|\n",
    "print(f'\\n{\"=\" * 60}')\n",
    "print('TOP FEATURES (sorted by |IC| at 15m):')\n",
    "print(f'{\"─\" * 60}')\n",
    "for feat, ic, sp, verdict in sorted(verdicts, key=lambda x: -x[1])[:10]:\n",
    "    print(f'  {verdict:4s} {feat:25s}  IC={ic:+.4f}  spread={sp:+.1f} bps')\n",
    "\n",
    "print(f'\\n→ Features marked ★★★ have both strong IC and tradeable spread (>15 bps)')\n",
    "print(f'→ Next step: combine top features into a composite signal and backtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4fdab5",
   "metadata": {},
   "source": [
    "# 02 — Signal Research: Microstructure Features from Tick Data\n",
    "\n",
    "**Goal:** Build features from raw tick trades that capture *what happened inside each candle* and evaluate their predictiveness for forward returns at 5m–1h horizons.\n",
    "\n",
    "**Feature Categories:**\n",
    "1. **Aggression** — taker buy/sell pressure, large trade detection\n",
    "2. **Flow patterns** — trade arrival rate, clustering, acceleration\n",
    "3. **Price impact** — how much price moves per unit of volume\n",
    "4. **Volume profile** — distribution of volume within the candle\n",
    "5. **Cross-exchange divergence** — when one venue sees unusual activity\n",
    "\n",
    "**Evaluation:** All signals tested against forward returns at 5m, 15m, 1h with a **minimum edge threshold of 15-20 bps** (to clear VIP0 fees + slippage).\n",
    "\n",
    "**Data:** BTCUSDT tick trades + OHLCV, 92 days, 6 sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

#!/usr/bin/env python3
"""
v42l: Exploit Cascade Momentum + Early Detection Ideas

EXP DD: Cascade Momentum Exploitation
  - After cascade #1, if another cascade comes within 15 min, it's 84-96%
    likely same direction. Strategy variants:
    1. "Double down": after first cascade trade, if second cascade same dir,
       enter AGAIN (stack positions)
    2. "Direction bias": after first cascade, only take trades in same direction
       for next 15 min (skip opposite)
    3. "Aggressive entry": after first cascade, use tighter offset for next
       cascade (faster fill, more trades)

EXP EE: Volume Spike as Cascade Early Warning
  - Does a sudden volume spike (5-15 sec before cascade) predict cascade?
  - If so, we can enter BEFORE the cascade is officially detected
  - Load tick-level volume data, check pre-cascade volume patterns

EXP FF: Cascade Size Prediction
  - Can we predict if a cascade will be large (P97+) vs small (P90-P95)?
  - Features: initial liquidation size, speed of first 2 events, time of day
  - If predictable, size position accordingly

88 days, all 3 symbols, RAM-safe.
"""

import sys, time, json, gzip, os, gc, psutil
from pathlib import Path
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
sys.stdout.reconfigure(line_buffering=True)

MAKER_FEE = 0.0002
TAKER_FEE = 0.00055


def ram_str():
    p = psutil.Process().memory_info().rss / 1024**3
    a = psutil.virtual_memory().available / 1024**3
    return f"RAM={p:.1f}GB, avail={a:.1f}GB"


class Tee:
    def __init__(self, fp):
        self.file = open(fp, 'w', buffering=1)
        self.stdout = sys.stdout
    def write(self, d):
        self.stdout.write(d)
        self.file.write(d)
    def flush(self):
        self.stdout.flush()
        self.file.flush()


def get_dates(start, n):
    base = datetime.strptime(start, '%Y-%m-%d')
    return [(base + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(n)]


def load_bars_chunked(symbol, dates, data_dir='data', chunk_days=10):
    base = Path(data_dir) / symbol / "bybit" / "futures"
    t0 = time.time(); n = len(dates)
    print(f"  Loading {symbol} bars...", end='', flush=True)
    all_bars = []
    for start in range(0, n, chunk_days):
        chunk_dates = dates[start:start+chunk_days]
        dfs = []
        for d in chunk_dates:
            f = base / f"{symbol}{d}.csv.gz"
            if f.exists():
                df = pd.read_csv(f, usecols=['timestamp', 'price'])
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
                dfs.append(df)
        if dfs:
            chunk = pd.concat(dfs, ignore_index=True)
            del dfs
            b = chunk.set_index('timestamp')['price'].resample('1min').agg(
                open='first', high='max', low='min', close='last').dropna()
            all_bars.append(b)
            del chunk; gc.collect()
        done = min(start+chunk_days, n)
        el = time.time()-t0
        print(f" [{done}/{n} {el:.0f}s]", end='', flush=True)
    if not all_bars: print(" NO DATA"); return pd.DataFrame()
    result = pd.concat(all_bars).sort_index()
    result = result[~result.index.duplicated(keep='first')]
    print(f" {len(result):,} bars ({time.time()-t0:.0f}s) [{ram_str()}]")
    return result


def load_liqs(symbol, dates, data_dir='data'):
    base = Path(data_dir) / symbol / "bybit" / "liquidations"
    t0 = time.time(); n = len(dates)
    print(f"  Loading {symbol} liqs...", end='', flush=True)
    recs = []
    for i, d in enumerate(dates):
        for hr in range(24):
            f = base / f"liquidation_{d}_hr{hr:02d}.jsonl.gz"
            if not f.exists(): continue
            with gzip.open(f, 'rt') as fh:
                for line in fh:
                    try:
                        data = json.loads(line)
                        if 'result' in data and 'data' in data['result']:
                            for ev in data['result']['data']:
                                recs.append({
                                    'timestamp': pd.to_datetime(ev['T'], unit='ms'),
                                    'side': ev['S'], 'volume': float(ev['v']),
                                    'price': float(ev['p']),
                                })
                    except: continue
        if (i+1) % 15 == 0:
            el = time.time()-t0
            print(f" [{i+1}/{n} {el:.0f}s]", end='', flush=True)
    if not recs: print(" NO DATA"); return pd.DataFrame()
    df = pd.DataFrame(recs).sort_values('timestamp').reset_index(drop=True)
    df['notional'] = df['volume'] * df['price']
    print(f" {len(df):,} ({time.time()-t0:.0f}s) [{ram_str()}]")
    return df


def detect_cascades_detailed(liq_df, pct_thresh=95, window=60, min_ev=2):
    """Returns cascades with extra detail: n_events, duration, first_event_size."""
    if liq_df.empty: return []
    vol_thresh = liq_df['notional'].quantile(pct_thresh / 100)
    large = liq_df[liq_df['notional'] >= vol_thresh]
    cascades = []
    current = []
    for _, row in large.iterrows():
        if not current: current = [row]
        else:
            dt = (row['timestamp'] - current[-1]['timestamp']).total_seconds()
            if dt <= window: current.append(row)
            else:
                if len(current) >= min_ev:
                    cdf = pd.DataFrame(current)
                    bn = cdf[cdf['side']=='Buy']['notional'].sum()
                    sn = cdf[cdf['side']=='Sell']['notional'].sum()
                    dur = (cdf['timestamp'].max() - cdf['timestamp'].min()).total_seconds()
                    cascades.append({
                        'start': cdf['timestamp'].min(), 'end': cdf['timestamp'].max(),
                        'total_notional': bn+sn, 'buy_dominant': bn > sn,
                        'n_events': len(cdf), 'duration_s': dur,
                        'first_notional': cdf.iloc[0]['notional'],
                    })
                current = [row]
    if len(current) >= min_ev:
        cdf = pd.DataFrame(current)
        bn = cdf[cdf['side']=='Buy']['notional'].sum()
        sn = cdf[cdf['side']=='Sell']['notional'].sum()
        dur = (cdf['timestamp'].max() - cdf['timestamp'].min()).total_seconds()
        cascades.append({
            'start': cdf['timestamp'].min(), 'end': cdf['timestamp'].max(),
            'total_notional': bn+sn, 'buy_dominant': bn > sn,
            'n_events': len(cdf), 'duration_s': dur,
            'first_notional': cdf.iloc[0]['notional'],
        })
    return cascades


def run_trail(cascades, bars, offset=0.15, tp=0.15, sl=0.50, max_hold=30, cooldown=300,
              trail_act=3, trail_dist=2):
    trades = []
    last_time = None
    for c in cascades:
        if last_time and (c['end'] - last_time).total_seconds() < cooldown: continue
        idx = bars.index.searchsorted(c['end'])
        if idx >= len(bars) - max_hold or idx < 1: continue
        price = bars.iloc[idx]['close']
        is_long = c['buy_dominant']
        if is_long:
            lim = price*(1-offset/100); tp_p = lim*(1+tp/100); sl_p = lim*(1-sl/100)
        else:
            lim = price*(1+offset/100); tp_p = lim*(1-tp/100); sl_p = lim*(1+sl/100)
        filled = False
        for j in range(idx, min(idx+max_hold, len(bars))):
            b = bars.iloc[j]
            if is_long and b['low'] <= lim: filled=True; fi=j; break
            elif not is_long and b['high'] >= lim: filled=True; fi=j; break
        if not filled: continue
        ep = None; er = 'timeout'
        best_profit = 0; trailing_active = False; current_sl = sl_p
        for k in range(fi, min(fi+max_hold, len(bars))):
            b = bars.iloc[k]
            if is_long:
                cp = (b['high']-lim)/lim
                if cp > best_profit: best_profit = cp
                if best_profit >= trail_act/10000 and not trailing_active:
                    trailing_active = True; current_sl = lim*(1+trail_dist/10000)
                if trailing_active:
                    ns = b['high']*(1-trail_dist/10000)
                    if ns > current_sl: current_sl = ns
                if b['low'] <= current_sl: ep=current_sl; er='trail' if trailing_active else 'sl'; break
                if b['high'] >= tp_p: ep=tp_p; er='tp'; break
            else:
                cp = (lim-b['low'])/lim
                if cp > best_profit: best_profit = cp
                if best_profit >= trail_act/10000 and not trailing_active:
                    trailing_active = True; current_sl = lim*(1-trail_dist/10000)
                if trailing_active:
                    ns = b['low']*(1+trail_dist/10000)
                    if ns < current_sl: current_sl = ns
                if b['high'] >= current_sl: ep=current_sl; er='trail' if trailing_active else 'sl'; break
                if b['low'] <= tp_p: ep=tp_p; er='tp'; break
        if ep is None: ep = bars.iloc[min(fi+max_hold, len(bars)-1)]['close']
        if is_long: gross = (ep-lim)/lim
        else: gross = (lim-ep)/lim
        fee = MAKER_FEE + (MAKER_FEE if er=='tp' else TAKER_FEE)
        trades.append({'net': gross-fee, 'exit': er, 'time': bars.index[fi],
                       'direction': 'long' if is_long else 'short',
                       'cascade_end': c['end'],
                       'notional': c['total_notional'],
                       'n_events': c.get('n_events', 0)})
        last_time = c['end']
    return trades


def pstats(trades, label):
    if not trades:
        print(f"    {label:55s}  NO TRADES"); return None
    arr = np.array([t['net'] for t in trades])
    n = len(arr); wr = (arr>0).mean()*100; avg = arr.mean()*10000
    tot = arr.sum()*100; std = arr.std()
    sh = arr.mean()/(std+1e-10)*np.sqrt(252*24*60)
    flag = "✅" if arr.mean() > 0 else "  "
    print(f"  {flag} {label:55s}  n={n:4d}  wr={wr:5.1f}%  avg={avg:+6.1f}bps  "
          f"tot={tot:+7.2f}%  sh={sh:+8.1f}")
    return {'n': n, 'wr': wr, 'avg': avg, 'tot': tot, 'sharpe': sh}


# ============================================================================
# EXP DD: MOMENTUM EXPLOITATION
# ============================================================================

def exp_dd_momentum(cascades, bars, symbol):
    print(f"\n{'='*80}")
    print(f"  EXP DD: CASCADE MOMENTUM EXPLOITATION — {symbol}")
    print(f"{'='*80}")

    # Sort cascades by time
    sorted_c = sorted(cascades, key=lambda c: c['end'])

    # Tag each cascade with "is_follow_up" and "same_dir_as_prev"
    for i, c in enumerate(sorted_c):
        if i == 0:
            c['is_followup'] = False
            c['same_dir'] = True
        else:
            gap = (c['end'] - sorted_c[i-1]['end']).total_seconds()
            c['is_followup'] = gap < 900  # within 15 min
            c['same_dir'] = c['buy_dominant'] == sorted_c[i-1]['buy_dominant']

    # Strategy 1: Direction bias — only take follow-up cascades if same direction
    print(f"\n  STRATEGY 1: Direction Bias (skip opposite-dir follow-ups)")
    biased = [c for c in sorted_c if not c['is_followup'] or c['same_dir']]
    unbiased = sorted_c

    t_unbiased = run_trail(unbiased, bars)
    t_biased = run_trail(biased, bars)
    pstats(t_unbiased, "Unbiased (all cascades)")
    pstats(t_biased, "Biased (skip opposite follow-ups)")

    # How many were skipped?
    opposite_followups = [c for c in sorted_c if c['is_followup'] and not c['same_dir']]
    print(f"  Opposite-dir follow-ups skipped: {len(opposite_followups)}")

    # Strategy 2: Aggressive offset on follow-ups
    print(f"\n  STRATEGY 2: Aggressive Entry on Follow-ups")
    # First cascades: normal offset 0.15%
    # Follow-up same-dir: tighter offset 0.10% (faster fill)
    first_c = [c for c in sorted_c if not c['is_followup']]
    followup_same = [c for c in sorted_c if c['is_followup'] and c['same_dir']]
    followup_opp = [c for c in sorted_c if c['is_followup'] and not c['same_dir']]

    t_first = run_trail(first_c, bars, offset=0.15)
    t_follow_tight = run_trail(followup_same, bars, offset=0.10)
    t_follow_normal = run_trail(followup_same, bars, offset=0.15)

    pstats(t_first, "First cascades (off=0.15)")
    pstats(t_follow_normal, "Follow-up same-dir (off=0.15)")
    pstats(t_follow_tight, "Follow-up same-dir (off=0.10, aggressive)")

    # Strategy 3: Reduced cooldown for same-dir follow-ups
    print(f"\n  STRATEGY 3: Reduced Cooldown for Same-Dir Follow-ups")
    for cd in [300, 180, 120, 60, 30]:
        t = run_trail(sorted_c, bars, cooldown=cd)
        pstats(t, f"cooldown={cd}s")


# ============================================================================
# EXP FF: CASCADE SIZE PREDICTION
# ============================================================================

def exp_ff_size_prediction(cascades, bars, symbol):
    print(f"\n{'='*80}")
    print(f"  EXP FF: CASCADE SIZE PREDICTION — {symbol}")
    print(f"{'='*80}")

    # Can we predict large cascades from first event?
    notionals = [c['total_notional'] for c in cascades]
    p50 = np.percentile(notionals, 50)
    p75 = np.percentile(notionals, 75)
    p90 = np.percentile(notionals, 90)

    # Feature: first event notional
    first_nots = [c['first_notional'] for c in cascades]
    fp50 = np.percentile(first_nots, 50)
    fp75 = np.percentile(first_nots, 75)

    print(f"  Cascade notional: P50={p50:.0f}, P75={p75:.0f}, P90={p90:.0f}")
    print(f"  First event notional: P50={fp50:.0f}, P75={fp75:.0f}")

    # Does large first event predict large cascade?
    large_first = [c for c in cascades if c['first_notional'] >= fp75]
    small_first = [c for c in cascades if c['first_notional'] < fp50]

    large_cascade_rate_big_first = sum(1 for c in large_first if c['total_notional'] >= p75) / len(large_first) * 100
    large_cascade_rate_small_first = sum(1 for c in small_first if c['total_notional'] >= p75) / len(small_first) * 100

    print(f"\n  FIRST EVENT SIZE → CASCADE SIZE:")
    print(f"    Large first (>P75): {large_cascade_rate_big_first:.1f}% become large cascades (>P75)")
    print(f"    Small first (<P50): {large_cascade_rate_small_first:.1f}% become large cascades (>P75)")

    # Trade large-first vs small-first cascades
    print(f"\n  TRADING BY FIRST EVENT SIZE:")
    t_large = run_trail(large_first, bars)
    t_small = run_trail(small_first, bars)
    pstats(t_large, "Large first event (>P75)")
    pstats(t_small, "Small first event (<P50)")

    # Feature: n_events
    print(f"\n  TRADING BY N_EVENTS:")
    for min_n in [2, 3, 4, 5]:
        sub = [c for c in cascades if c['n_events'] >= min_n]
        t = run_trail(sub, bars)
        pstats(t, f"n_events >= {min_n} ({len(sub)} cascades)")

    # Feature: duration
    print(f"\n  TRADING BY CASCADE DURATION:")
    for max_dur in [10, 20, 30, 45, 60]:
        sub = [c for c in cascades if c['duration_s'] <= max_dur]
        t = run_trail(sub, bars)
        pstats(t, f"duration <= {max_dur}s ({len(sub)} cascades)")

    # Feature: hour of day
    print(f"\n  TRADING BY HOUR OF DAY:")
    hour_stats = {}
    for c in cascades:
        hr = c['end'].hour
        if hr not in hour_stats: hour_stats[hr] = []
        hour_stats[hr].append(c)
    for hr in sorted(hour_stats.keys()):
        t = run_trail(hour_stats[hr], bars)
        if t:
            arr = np.array([x['net'] for x in t])
            avg = arr.mean() * 10000
            flag = "✅" if avg > 0 else "  "
            print(f"  {flag} hr={hr:02d}  n={len(t):3d}  avg={avg:+5.1f}bps")


# ============================================================================
# MAIN
# ============================================================================

def main():
    out_file = 'results/v42l_momentum_exploit.txt'
    os.makedirs('results', exist_ok=True)
    tee = Tee(out_file)
    sys.stdout = tee
    t0 = time.time()

    symbols = ['ETHUSDT', 'SOLUSDT', 'DOGEUSDT']
    all_dates = get_dates('2025-05-12', 88)

    print("="*80)
    print(f"  v42l: MOMENTUM EXPLOITATION + SIZE PREDICTION")
    print(f"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  [{ram_str()}]")
    print("="*80)

    # Load data
    all_liq = {}
    for sym in symbols:
        all_liq[sym] = load_liqs(sym, all_dates)
    gc.collect()

    all_bars = {}
    for sym in symbols:
        all_bars[sym] = load_bars_chunked(sym, all_dates, chunk_days=10)
        gc.collect()

    print(f"\n  [{ram_str()}] all data loaded")

    # Run experiments per symbol
    for sym in symbols:
        cascades = detect_cascades_detailed(all_liq[sym], pct_thresh=95)
        print(f"\n  {sym}: {len(cascades)} cascades")

        exp_dd_momentum(cascades, all_bars[sym], sym)
        exp_ff_size_prediction(cascades, all_bars[sym], sym)

    elapsed = time.time() - t0
    print(f"\n{'#'*80}")
    print(f"  COMPLETE — {elapsed:.0f}s ({elapsed/60:.1f}min) [{ram_str()}]")
    print(f"{'#'*80}")


if __name__ == '__main__':
    main()
